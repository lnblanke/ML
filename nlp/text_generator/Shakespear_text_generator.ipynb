{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cb8d35-9a88-4fae-9129-14cb9a680b26",
   "metadata": {},
   "source": [
    "# Shakespear Text Generator\n",
    "\n",
    "This notebook uses RNN to generate Shakespear-like texts, adapted from [example of RNN in Hands on Machine Learning](https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be686e-7b20-4a9c-a061-859eade1a2fe",
   "metadata": {},
   "source": [
    "We first import all the dependent libraries from training. In this notebook, we use TensorFlow to build our model. We also need to open input file containing text written by Shakespear for training, acquired from [karpathy's repository](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f894d2b-1813-473e-b99f-a969b8ad59a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 09:11:29.461807: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-02 09:11:30.944650: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-02 09:11:31.097950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1731] Found device 0 with properties: \n",
      "pciBusID: 0004:05:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.00GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-01-02 09:11:31.097982: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-02 09:11:31.103014: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-01-02 09:11:31.103058: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-01-02 09:11:31.105463: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-02 09:11:31.107205: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-02 09:11:31.109467: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-01-02 09:11:31.111682: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-01-02 09:11:31.113340: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-01-02 09:11:31.116056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\n",
      "2022-01-02 09:11:31.116082: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-01-02 09:11:31.493758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1256] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-02 09:11:31.493814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1262]      0 \n",
      "2022-01-02 09:11:31.493826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1275] 0:   N \n",
      "2022-01-02 09:11:31.497867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1416] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14674 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:05:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb844e0-139e-472f-b892-017a1bbd7fce",
   "metadata": {},
   "source": [
    "We then tokenize the text in char level, representing each character with a single number(ID). We substract 1 from the original vector to ensure that the minimum ID is 0 rather than 1. After that, we build a TensorFlow dataset for training with first 90% data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f079682c-6cee-4f00-993e-719c4cfb9e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 09:11:33.683847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1731] Found device 0 with properties: \n",
      "pciBusID: 0004:05:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.00GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-01-02 09:11:33.687264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\n",
      "2022-01-02 09:11:33.689274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1731] Found device 0 with properties: \n",
      "pciBusID: 0004:05:00.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.00GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-01-02 09:11:33.692519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\n",
      "2022-01-02 09:11:33.692570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1256] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-02 09:11:33.692582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1262]      0 \n",
      "2022-01-02 09:11:33.692594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1275] 0:   N \n",
      "2022-01-02 09:11:33.695890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1416] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14674 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:05:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "[token] = np.array(tokenizer.texts_to_sequences([text])) - 1\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    token[:int(tokenizer.document_count * .9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a22b1-0d2a-4c79-b33a-b2c20339d0b8",
   "metadata": {},
   "source": [
    "Here we define several constants important for training. We set ```steps``` to 100, meaning we are using 100 characters to predict the next character. We use ```w_len``` for window length, which also contains another character in the right as the target, while ```char_size``` is the number of possible ID values. For batch size, we use a relatively large value because the dataset contains millions of values and a large batch size decreases training time. After that, we apply a series of transformations to the dataset. First, we use ```window``` to crop the dataset into windows of ```w_len``` and use ```flat_map``` to flatten the cropped dataset. After shuffling and batching, we separate dataset into data and label and transform data to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a02b4c-1144-442e-b78b-ce824d3372d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 100\n",
    "w_len = steps + 1\n",
    "batch_size = 1024\n",
    "char_size = len(tokenizer.word_index)\n",
    "\n",
    "ds = train_ds.window(w_len, shift = 1, drop_remainder = True)\n",
    "ds = ds.flat_map(lambda w : w.batch(w_len))\n",
    "ds = ds.shuffle(steps * 100).batch(batch_size)\n",
    "ds = ds.map(lambda w: (w[:, :-1], w[:, 1:]))\n",
    "ds = ds.map(lambda x, y: (tf.one_hot(x, depth = char_size), y)).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eebf6b-23de-4224-a833-fcb730b6ab0c",
   "metadata": {},
   "source": [
    "We can then build up our model. Our model is very simple, containing 2 GRU layers and a Softmax layer for output. We set ```current_dropout``` to 0 so that GRUs can be supported by GPU. We compile the model with Adam optimizer and train it on the dataset for 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ccb48d-0c73-4cce-87cf-8ef4a847ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 09:11:34.108068: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-02 09:11:34.160927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3783000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-02 09:11:38.850257: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-01-02 09:11:39.388221: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8101\n",
      "2022-01-02 09:11:39.736807: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-01-02 09:11:40.196298: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981/981 [==============================] - 210s 208ms/step - loss: 2.2321\n",
      "Epoch 2/20\n",
      "981/981 [==============================] - 206s 208ms/step - loss: 1.8896\n",
      "Epoch 3/20\n",
      "981/981 [==============================] - 203s 205ms/step - loss: 1.8101\n",
      "Epoch 4/20\n",
      "981/981 [==============================] - 204s 206ms/step - loss: 1.7718\n",
      "Epoch 5/20\n",
      "981/981 [==============================] - 203s 205ms/step - loss: 1.7480\n",
      "Epoch 6/20\n",
      "981/981 [==============================] - 204s 206ms/step - loss: 1.7313\n",
      "Epoch 7/20\n",
      "981/981 [==============================] - 204s 206ms/step - loss: 1.7195\n",
      "Epoch 8/20\n",
      "981/981 [==============================] - 204s 206ms/step - loss: 1.7105\n",
      "Epoch 9/20\n",
      "981/981 [==============================] - 203s 206ms/step - loss: 1.7030\n",
      "Epoch 10/20\n",
      "981/981 [==============================] - 204s 206ms/step - loss: 1.6968\n",
      "Epoch 11/20\n",
      "981/981 [==============================] - 203s 205ms/step - loss: 1.6916\n",
      "Epoch 12/20\n",
      "981/981 [==============================] - 203s 205ms/step - loss: 1.6875\n",
      "Epoch 13/20\n",
      "981/981 [==============================] - 203s 205ms/step - loss: 1.6834\n",
      "Epoch 14/20\n",
      "981/981 [==============================] - 202s 204ms/step - loss: 1.6801\n",
      "Epoch 15/20\n",
      "981/981 [==============================] - 204s 206ms/step - loss: 1.6774\n",
      "Epoch 16/20\n",
      "981/981 [==============================] - 203s 206ms/step - loss: 1.6747\n",
      "Epoch 17/20\n",
      "981/981 [==============================] - 203s 205ms/step - loss: 1.6723\n",
      "Epoch 18/20\n",
      "981/981 [==============================] - 202s 204ms/step - loss: 1.6698\n",
      "Epoch 19/20\n",
      "981/981 [==============================] - 203s 205ms/step - loss: 1.6679\n",
      "Epoch 20/20\n",
      "981/981 [==============================] - 203s 205ms/step - loss: 1.6658\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                             tf.keras.layers.GRU(128, return_sequences = True, \n",
    "                                                 dropout = .2, \n",
    "                                                 recurrent_dropout = 0),\n",
    "                             tf.keras.layers.GRU(128, return_sequences = True, \n",
    "                                                 dropout = .2, \n",
    "                                                 recurrent_dropout = 0),\n",
    "                             tf.keras.layers.TimeDistributed(\n",
    "                                 tf.keras.layers.Dense(char_size, \n",
    "                                                activation = \"softmax\")\n",
    "                             )\n",
    "])\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\")\n",
    "history = model.fit(ds, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad553df-fa16-47a3-ae7b-c6ea905f1b16",
   "metadata": {},
   "source": [
    "Here we define a function to preprocess the text so that it can be fed into the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa736db-7503-4161-a1c2-851c9612641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  x = np.array(tokenizer.texts_to_sequences(text)) - 1\n",
    "\n",
    "  return tf.one_hot(x, char_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7996d-b91a-4b45-b59b-12e60bcbc2f8",
   "metadata": {},
   "source": [
    "We test the model with a simple sentence. The output should be 'u'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c397adc0-0295-4877-ae89-44dd733f9c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\n"
     ]
    }
   ],
   "source": [
    "sample_x = preprocess([\"How are yo\"])\n",
    "sample_pred = model.predict(sample_x)\n",
    "print(tokenizer.sequences_to_texts(np.argmax(sample_pred, axis = -1) + 1)[0][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967aa9a5-7d6f-4271-8e5d-966dc7f435e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to generate a series of characters, we need to define two functions. In ```next_char```, we use the text given to predict the next character ID and transform it into the char predicted. In this function, we use two methods to avoid repeating characters. With ```tf.random.categorical``` function, we can generate random characters based on probabilities predicted. We also define a parameter ```temp``` as temperature, which controls how much the generator flavors high-prob characters. In ```complete```, we simply add all the characters generated together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88459f6-7199-4532-b776-83ba3f3c5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temp = 1):\n",
    "  x = preprocess([text])\n",
    "  y = model.predict(x)[0, -1:, :]\n",
    "  logits = tf.math.log(y) / temp\n",
    "  char = tf.random.categorical(logits, num_samples = 1) + 1\n",
    "  return tokenizer.sequences_to_texts(char.numpy())[0]\n",
    "\n",
    "def complete(text, n_char = 50, temp = 1):\n",
    "  for _ in range(n_char):\n",
    "    text += next_char(text, temp)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55ff88-0e0f-4f0b-8b91-a0bef422707d",
   "metadata": {},
   "source": [
    "In our last step, we can finally generate text in Shakespear style. Starting with a letter T, what can our model generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f336ffe-10e6-4e95-b8b0-4483ad01f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T sir.\n",
      "\n",
      "arswers:\n",
      "percupio, shr she down as iron, go\n"
     ]
    }
   ],
   "source": [
    "print(complete(\"T\", temp = 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-env]",
   "language": "python",
   "name": "conda-env-.conda-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
