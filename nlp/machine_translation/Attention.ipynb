{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41163,
     "status": "ok",
     "timestamp": 1641050431809,
     "user": {
      "displayName": "David Fu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08135212940790007194"
     },
     "user_tz": 360
    },
    "id": "s5s0wPclNXrn",
    "outputId": "ae46fc4b-82b8-4fd0-c4e7-60430f8c12c7"
   },
   "outputs": [],
   "source": [
    "! pip install -r ../requirements.txt\n",
    "\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tftext\n",
    "import numpy as np\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjVv_DhdPbAJ"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path.read_text(encoding='utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  text = lines\n",
    "\n",
    "  return text\n",
    "\n",
    "train_targ = load_data(pathlib.Path(\"data/train.seg.zh.txt\"))\n",
    "train_inp = load_data(pathlib.Path(\"data/train.seg.en.txt\"))\n",
    "val_targ = load_data(pathlib.Path(\"data/dev.seg.zh.txt\"))\n",
    "val_inp = load_data(pathlib.Path(\"data/dev.seg.en.txt\"))\n",
    "test_targ = load_data(pathlib.Path(\"data/test.seg.zh.txt\"))\n",
    "test_inp = load_data(pathlib.Path(\"data/test.seg.en.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRrO2q94vH9T"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    train_inp, train_targ)\n",
    ").shuffle(len(train_targ)).batch(batch_size)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     val_inp, val_targ)\n",
    "# ).shuffle(len(val_targ)).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    test_inp, test_targ)\n",
    ").shuffle(len(test_targ)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "pB4HXfrVCI92"
   },
   "outputs": [],
   "source": [
    "def inp_preprocess(text):\n",
    "  text = tftext.normalize_utf8(text, \"NFKD\")\n",
    "  text = tf.strings.lower(text)\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.regex_replace(text, '[/(){}:;<>,|\\'\"]', '')\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator = ' ')\n",
    "\n",
    "  return text\n",
    "\n",
    "def targ_preprocess(text):\n",
    "  text = tftext.normalize_utf8(text, \"NFKD\")\n",
    "  text = tf.strings.lower(text)\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.regex_replace(text, '[/(){}:;<>,|\\'\"]', '')\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator = ' ')\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6451,
     "status": "ok",
     "timestamp": 1641051392589,
     "user": {
      "displayName": "David Fu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08135212940790007194"
     },
     "user_tz": 360
    },
    "id": "kqrOOJZAyXZh",
    "outputId": "12adc79c-b0bc-4025-82a1-3c6b1f32ace6"
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "\n",
    "input_processor = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize = inp_preprocess, max_tokens = max_vocab_size)\n",
    "output_processor = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize = targ_preprocess, max_tokens = max_vocab_size)\n",
    "input_processor.adapt(train_inp)\n",
    "output_processor.adapt(train_targ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a valid call:\n",
      "tf.Tensor(b'[START] the following is a valid call [END]', shape=(), dtype=string)\n",
      "['以下 均 为 合法 的 调用 :', '3.7   新版 功能 .', '后者 相应 增加 了 一个 别名 :   \" Screen . onkeyrelease ( ) \"', '\" %% \"   字面 的   \" \\' % \\' \"   字符', '子 进程 协议', '请 注意   cursor   的   arraysize   属性 会 影响 此 操作 的 执行 效率', 'state   是 一个 表示 编码器 状态 的 元组', '\" tarfile . open ( ) \"   函数 实际上 是 这个 类 方法 的 快捷方式', '任何 时候 将   \" NULL \"   指针 “ 泄露 ” 给   Python   用户 都 会 是 个 严重 的 错误', '可以 将 所有 数值 设置 为   \" CHAR _ MAX \"   ， 以 指示 此 语言 环境 中 未指定 任何 值']\n",
      "tf.Tensor(\n",
      "[b'[START] \\xe4\\xbb\\xa5\\xe4\\xb8\\x8b \\xe5\\x9d\\x87 \\xe4\\xb8\\xba \\xe5\\x90\\x88\\xe6\\xb3\\x95 \\xe7\\x9a\\x84 \\xe8\\xb0\\x83\\xe7\\x94\\xa8  [END]'\n",
      " b'[START] 3.7   \\xe6\\x96\\xb0\\xe7\\x89\\x88 \\xe5\\x8a\\x9f\\xe8\\x83\\xbd . [END]'\n",
      " b'[START] \\xe5\\x90\\x8e\\xe8\\x80\\x85 \\xe7\\x9b\\xb8\\xe5\\xba\\x94 \\xe5\\xa2\\x9e\\xe5\\x8a\\xa0 \\xe4\\xba\\x86 \\xe4\\xb8\\x80\\xe4\\xb8\\xaa \\xe5\\x88\\xab\\xe5\\x90\\x8d     screen . onkeyrelease    [END]'\n",
      " b'[START]  %%    \\xe5\\xad\\x97\\xe9\\x9d\\xa2 \\xe7\\x9a\\x84     %     \\xe5\\xad\\x97\\xe7\\xac\\xa6 [END]'\n",
      " b'[START] \\xe5\\xad\\x90 \\xe8\\xbf\\x9b\\xe7\\xa8\\x8b \\xe5\\x8d\\x8f\\xe8\\xae\\xae [END]'\n",
      " b'[START] \\xe8\\xaf\\xb7 \\xe6\\xb3\\xa8\\xe6\\x84\\x8f   cursor   \\xe7\\x9a\\x84   arraysize   \\xe5\\xb1\\x9e\\xe6\\x80\\xa7 \\xe4\\xbc\\x9a \\xe5\\xbd\\xb1\\xe5\\x93\\x8d \\xe6\\xad\\xa4 \\xe6\\x93\\x8d\\xe4\\xbd\\x9c \\xe7\\x9a\\x84 \\xe6\\x89\\xa7\\xe8\\xa1\\x8c \\xe6\\x95\\x88\\xe7\\x8e\\x87 [END]'\n",
      " b'[START] state   \\xe6\\x98\\xaf \\xe4\\xb8\\x80\\xe4\\xb8\\xaa \\xe8\\xa1\\xa8\\xe7\\xa4\\xba \\xe7\\xbc\\x96\\xe7\\xa0\\x81\\xe5\\x99\\xa8 \\xe7\\x8a\\xb6\\xe6\\x80\\x81 \\xe7\\x9a\\x84 \\xe5\\x85\\x83\\xe7\\xbb\\x84 [END]'\n",
      " b'[START]  tarfile . open      \\xe5\\x87\\xbd\\xe6\\x95\\xb0 \\xe5\\xae\\x9e\\xe9\\x99\\x85\\xe4\\xb8\\x8a \\xe6\\x98\\xaf \\xe8\\xbf\\x99\\xe4\\xb8\\xaa \\xe7\\xb1\\xbb \\xe6\\x96\\xb9\\xe6\\xb3\\x95 \\xe7\\x9a\\x84 \\xe5\\xbf\\xab\\xe6\\x8d\\xb7\\xe6\\x96\\xb9\\xe5\\xbc\\x8f [END]'\n",
      " b'[START] \\xe4\\xbb\\xbb\\xe4\\xbd\\x95 \\xe6\\x97\\xb6\\xe5\\x80\\x99 \\xe5\\xb0\\x86    null    \\xe6\\x8c\\x87\\xe9\\x92\\x88 \\xe2\\x80\\x9c \\xe6\\xb3\\x84\\xe9\\x9c\\xb2 \\xe2\\x80\\x9d \\xe7\\xbb\\x99   python   \\xe7\\x94\\xa8\\xe6\\x88\\xb7 \\xe9\\x83\\xbd \\xe4\\xbc\\x9a \\xe6\\x98\\xaf \\xe4\\xb8\\xaa \\xe4\\xb8\\xa5\\xe9\\x87\\x8d \\xe7\\x9a\\x84 \\xe9\\x94\\x99\\xe8\\xaf\\xaf [END]'\n",
      " b'[START] \\xe5\\x8f\\xaf\\xe4\\xbb\\xa5 \\xe5\\xb0\\x86 \\xe6\\x89\\x80\\xe6\\x9c\\x89 \\xe6\\x95\\xb0\\xe5\\x80\\xbc \\xe8\\xae\\xbe\\xe7\\xbd\\xae \\xe4\\xb8\\xba    char _ max     \\xe4\\xbb\\xa5 \\xe6\\x8c\\x87\\xe7\\xa4\\xba \\xe6\\xad\\xa4 \\xe8\\xaf\\xad\\xe8\\xa8\\x80 \\xe7\\x8e\\xaf\\xe5\\xa2\\x83 \\xe4\\xb8\\xad \\xe6\\x9c\\xaa\\xe6\\x8c\\x87\\xe5\\xae\\x9a \\xe4\\xbb\\xbb\\xe4\\xbd\\x95 \\xe5\\x80\\xbc [END]'], shape=(10,), dtype=string)\n",
      "['', '[UNK]', '[START]', '[END]', '的', '在', '.', '一个', '_', '将', '是', '对象', '和', '中', '为', '使用', '被', '如果', '返回', 'python']\n",
      "10000\n",
      "9775\n"
     ]
    }
   ],
   "source": [
    "print(train_inp[0])\n",
    "print(targ_preprocess(train_inp[0]))\n",
    "print(train_targ[:10])\n",
    "print(targ_preprocess(train_targ[:10]))\n",
    "print(output_processor.get_vocabulary()[:20])\n",
    "print(input_processor.vocabulary_size())\n",
    "print(output_processor.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIAaLMLDdR_h"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqfqDvU-ePML"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_size, embedding_dim, units):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.units = units\n",
    "    self.input_size = input_size\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                   return_sequences = True, \n",
    "                                   return_state = True, \n",
    "                                   recurrent_initializer = \"glorot_uniform\")\n",
    "\n",
    "  def call(self, token, state = None):\n",
    "    vector = self.embedding(token)\n",
    "\n",
    "    out, state = self.gru(vector, initial_state = state)\n",
    "\n",
    "    return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6Oon_-9on1M"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "\n",
    "    self.w1 = tf.keras.layers.Dense(units, use_bias = False)\n",
    "    self.w2 = tf.keras.layers.Dense(units, use_bias = False)\n",
    "\n",
    "    self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "  def call(self, q, v, mask):\n",
    "    query = self.w1(q)\n",
    "    key = self.w2(v)\n",
    "\n",
    "    query_mask = tf.ones(tf.shape(query)[:-1], dtype = bool)\n",
    "    value_mask = mask\n",
    "\n",
    "    context_vector, weights = self.attention(inputs = [query, v, key], \n",
    "                                             mask = [query_mask, mask], \n",
    "                                             return_attention_scores = True)\n",
    "\n",
    "    return context_vector, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10xmU8s7qcbr"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "class DecoderInput(typing.NamedTuple):\n",
    "  new_tokens: typing.Any\n",
    "  enc_output: typing.Any\n",
    "  mask: typing.Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "  logits: typing.Any\n",
    "  attention_weights: typing.Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_tYEWJyer44"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_size, embedding_dim, units):\n",
    "      super(Decoder, self).__init__()\n",
    "\n",
    "      self.output_size = output_size\n",
    "      self.embedding_dim = embedding_dim\n",
    "      self.units = units\n",
    "\n",
    "      self.embedding = tf.keras.layers.Embedding(\n",
    "          self.output_size, \n",
    "          embedding_dim)\n",
    "      self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                     return_sequences = True, \n",
    "                                     return_state = True, \n",
    "                                     recurrent_initializer = 'glorot_uniform')\n",
    "      self.attention= Attention(self.units)\n",
    "\n",
    "      self.w = tf.keras.layers.Dense(self.units, \n",
    "                                     activation = \"tanh\", \n",
    "                                     use_bias = False)\n",
    "      self.fc = tf.keras.layers.Dense(self.output_size)\n",
    "\n",
    "  def call(self, \n",
    "           input: DecoderInput, \n",
    "           state = None) -> typing.Tuple[DecoderOutput, tf.Tensor]:\n",
    "      vec = self.embedding(input.new_tokens)\n",
    "      out, state = self.gru(vec, initial_state = state)\n",
    "\n",
    "      context, weights = self.attention(out, \n",
    "                                        input.enc_output, \n",
    "                                        mask = input.mask)\n",
    "\n",
    "      attention_vector = self.w(tf.concat([context, out], axis = -1))\n",
    "\n",
    "      logits = self.fc(attention_vector)\n",
    "\n",
    "      return DecoderOutput(logits, weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkQeCIU5zbYJ"
   },
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = \"masked_loss\"\n",
    "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits = True, \n",
    "        reduction = \"none\")\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    loss = self.loss(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srhA89bI1vO9"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, input_processor, output_processor):\n",
    "    super().__init__()\n",
    "\n",
    "    self.encoder = Encoder(input_processor.vocabulary_size(), \n",
    "                           embedding_dim, \n",
    "                           units)\n",
    "    self.decoder = Decoder(output_processor.vocabulary_size(), \n",
    "                           embedding_dim, \n",
    "                           units)\n",
    "\n",
    "    self.input_processor = input_processor\n",
    "    self.output_processor = output_processor\n",
    "\n",
    "  def train_step(self, input):\n",
    "    return self._tf_train_step(input)\n",
    "\n",
    "  def _preprocess(self, input, targ):\n",
    "    input_token = self.input_processor(input)\n",
    "    targ_token = self.output_processor(targ)\n",
    "\n",
    "    input_mask = input_token != 0\n",
    "    targ_mask = targ_token != 0\n",
    "\n",
    "    return input_token, input_mask, targ_token, targ_mask\n",
    "\n",
    "  def _train_step(self, inp):\n",
    "    input, targ = inp\n",
    "\n",
    "    input_token, input_mask, targ_token, targ_mask = self._preprocess(input, \n",
    "                                                                      targ)\n",
    "\n",
    "    max_length = tf.shape(targ_token)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      enc_out, enc_state = self.encoder(input_token)\n",
    "\n",
    "      dec_state = enc_state\n",
    "      loss = tf.constant(0.0)\n",
    "\n",
    "      for i in tf.range(max_length - 1):\n",
    "        new_token = targ_token[:, i : i + 2]\n",
    "        step_loss, dec_state = self._loop_state(new_token, \n",
    "                                                input_mask, \n",
    "                                                enc_out, \n",
    "                                                dec_state)\n",
    "\n",
    "        loss = loss + step_loss\n",
    "\n",
    "      avg_loss = loss / tf.reduce_sum(tf.cast(targ_mask, tf.float32))\n",
    "\n",
    "    var = self.trainable_variables\n",
    "    grad = tape.gradient(avg_loss, var)\n",
    "\n",
    "    self.optimizer.apply_gradients(zip(grad, var))\n",
    "\n",
    "    return {\"batch_loss\": avg_loss}\n",
    "\n",
    "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None, ]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None, ])]])\n",
    "  def _tf_train_step(self, input):\n",
    "    return self._train_step(input)\n",
    "\n",
    "  def _loop_state(self, new_token, input_mask, enc_out, dec_state):\n",
    "    input_token, targ_token = new_token[:, 0:1], new_token[:, 1:2]\n",
    "\n",
    "    dec_input = DecoderInput(input_token, enc_out, input_mask)\n",
    "\n",
    "    dec_out, dec_state = self.decoder(dec_input, dec_state)\n",
    "\n",
    "    step_loss = self.loss(targ_token, dec_out.logits)\n",
    "\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3388089,
     "status": "ok",
     "timestamp": 1641056367545,
     "user": {
      "displayName": "David Fu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08135212940790007194"
     },
     "user_tz": 360
    },
    "id": "X0SskFSmR6yk",
    "outputId": "7263c1b3-cd7e-4ce5-8708-a42cb4f79873"
   },
   "outputs": [],
   "source": [
    "translator = Translator(embedding_dim, units, input_processor, output_processor)\n",
    "\n",
    "translator.compile(optimizer = \"adam\", loss = MaskedLoss())\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor = \"batch_loss\", \n",
    "                                              patience = 5, \n",
    "                                              restore_best_weights = True)\n",
    "\n",
    "class BatchLog(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, key):\n",
    "    self.key = key\n",
    "    self.logs = []\n",
    "\n",
    "  def on_train_batch_end(self, n, logs):\n",
    "    self.logs.append(logs[self.key])\n",
    "\n",
    "bl = BatchLog(\"batch_loss\")\n",
    "\n",
    "translator.fit(train_dataset, epochs = 30, callbacks = [bl, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "RvPcJS-njWGd"
   },
   "outputs": [],
   "source": [
    "class Translate(tf.Module):\n",
    "  def __init__(self, encoder, decoder, input_processor, output_processor):\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.input_processor = input_processor\n",
    "    self.output_processor = output_processor\n",
    "\n",
    "    self.output_string_from_index = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "        vocabulary = output_processor.get_vocabulary(), mask_token = '', invert = True)\n",
    "    index_from_string = tf.keras.layers.StringLookup(vocabulary = output_processor.get_vocabulary(), mask_token = '')\n",
    "    token_mask_id = index_from_string([\"\", \"[UNK]\", \"[START]\"]).numpy()\n",
    "    self.token_mask = np.zeros([index_from_string.vocabulary_size()], dtype = np.bool)\n",
    "    self.token_mask[np.array(token_mask_id)] = True\n",
    "\n",
    "    self.start_token = index_from_string(tf.constant(\"[START]\"))\n",
    "    self.end_token = index_from_string(tf.constant(\"[END]\"))\n",
    "\n",
    "  def token_to_string(self, result_token):\n",
    "    result_text_token = self.output_string_from_index(result_token)\n",
    "    result_text = tf.strings.strip(tf.strings.reduce_join(result_text_token, axis = 1, separator = ' '))\n",
    "\n",
    "    return result_text\n",
    "\n",
    "  def sample(self, logits, temp):\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temp == 0.0:\n",
    "      new_token = tf.argmax(logits, axis = -1)\n",
    "    else: \n",
    "      logits = tf.squeeze(logits, axis = 1)\n",
    "      new_token = tf.random.categorical(logits / temp, num_samples = 1)\n",
    "\n",
    "    return new_token\n",
    "\n",
    "  def translate(self, input_text, *, max_l = 50, return_attention = True, temp = 1.0):\n",
    "    batch_size = tf.shape(input_text)[0]\n",
    "\n",
    "    input_token = self.input_processor(input_text)\n",
    "    enc_out, enc_state = self.encoder(input_token)\n",
    "\n",
    "    dec_state = enc_state\n",
    "    new_token = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "    result_token = []\n",
    "    attention = []\n",
    "\n",
    "    done = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
    "\n",
    "    for _ in range(max_l):\n",
    "      dec_in = DecoderInput(new_token, enc_out, (input_token != 0))\n",
    "\n",
    "      dec_result, dec_state = self.decoder(dec_in, state = dec_state)\n",
    "\n",
    "      attention.append(dec_result.attention_weights)\n",
    "\n",
    "      new_token = self.sample(dec_result.logits, temp)\n",
    "\n",
    "      done |= (new_token == self.end_token)\n",
    "\n",
    "      new_token = tf.where(done, tf.constant(0, dtype = tf.int64), new_token)\n",
    "\n",
    "      result_token.append(new_token)\n",
    "\n",
    "    result_token = tf.concat(result_token, axis = -1)\n",
    "    result_text = self.token_to_string(result_token)\n",
    "\n",
    "    if return_attention:\n",
    "      attention_stack = tf.concat(attention, axis = -1)\n",
    "      return {\"text\": result_text, \"attention:\": attention_stack}\n",
    "    else:\n",
    "      return {\"text\": result_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6795,
     "status": "ok",
     "timestamp": 1641056374336,
     "user": {
      "displayName": "David Fu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08135212940790007194"
     },
     "user_tz": 360
    },
    "id": "FffR8jft2NJe",
    "outputId": "c70eaddd-b9ea-41f5-d2ce-cd9fe817612f"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers' has no attribute 'StringLookup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1857631/841290779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_processor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msample_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1857631/3008440063.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, encoder, decoder, input_processor, output_processor)\u001b[0m\n\u001b[1;32m      8\u001b[0m     self.output_string_from_index = tf.keras.layers.experimental.preprocessing.StringLookup(\n\u001b[1;32m      9\u001b[0m         vocabulary = output_processor.get_vocabulary(), mask_token = '', invert = True)\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mindex_from_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringLookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtoken_mask_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[START]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_from_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'StringLookup'"
     ]
    }
   ],
   "source": [
    "translate = Translate(translator.encoder, translator.decoder, input_processor, output_processor)\n",
    "\n",
    "for i in range(10):\n",
    "  sample_input = tf.constant([test_inp[np.random.randint(0, tf.shape(test_inp)[0])]])\n",
    "\n",
    "  print(sample_input[0].numpy().decode())\n",
    "\n",
    "  result = translate.translate(sample_input)\n",
    "\n",
    "  print(tf.strings.regex_replace(result[\"text\"][0], ' ', '').numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNx8CfLN7wlVYuS3CS05uAv",
   "collapsed_sections": [],
   "name": "Attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-env]",
   "language": "python",
   "name": "conda-env-.conda-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
